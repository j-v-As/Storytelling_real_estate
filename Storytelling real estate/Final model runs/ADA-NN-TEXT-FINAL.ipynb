{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### final model NN TEXT ####\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\df-english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "df[\"ada_embedding_eng\"] = df[\"ada_embedding_eng\"].apply(literal_eval).apply(np.array)\n",
    "\n",
    "x = df[\"ada_embedding_eng\"]\n",
    "y = df['numerical_price']\n",
    "\n",
    "del df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(x), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (57611, 1536)\n",
      "x_test shape: (14403, 1536)\n",
      "y_train shape: (57611,)\n",
      "y_test shape: (14403,)\n",
      "Adjusted y_train shape: (57611,)\n",
      "Adjusted y_test shape: (14403,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "print(\"Adjusted y_train shape:\", y_train.shape)\n",
    "print(\"Adjusted y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "1801/1801 [==============================] - 32s 17ms/step - loss: 0.5121\n",
      "Epoch 2/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.4095\n",
      "Epoch 3/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.3695\n",
      "Epoch 4/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.3319\n",
      "Epoch 5/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.3068\n",
      "Epoch 6/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2732\n",
      "Epoch 7/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.2550\n",
      "Epoch 8/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.2320\n",
      "Epoch 9/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.2239\n",
      "Epoch 10/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.2042\n",
      "Epoch 11/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.1926\n",
      "Epoch 12/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1836\n",
      "Epoch 13/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.1705\n",
      "Epoch 14/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1628\n",
      "Epoch 15/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1608\n",
      "Epoch 16/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1532\n",
      "Epoch 17/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.1445\n",
      "Epoch 18/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1504\n",
      "Epoch 19/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1374\n",
      "Epoch 20/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1258\n",
      "Epoch 21/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1215\n",
      "Epoch 22/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1143\n",
      "Epoch 23/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1087\n",
      "Epoch 24/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1080\n",
      "Epoch 25/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1110\n",
      "Epoch 26/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1047\n",
      "Epoch 27/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0941\n",
      "Epoch 28/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0935\n",
      "Epoch 29/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0917\n",
      "Epoch 30/66\n",
      "1801/1801 [==============================] - 37s 21ms/step - loss: 0.0876\n",
      "Epoch 31/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0797\n",
      "Epoch 32/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0756\n",
      "Epoch 33/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0775\n",
      "Epoch 34/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0826\n",
      "Epoch 35/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0705\n",
      "Epoch 36/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0666\n",
      "Epoch 37/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0686\n",
      "Epoch 38/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0670\n",
      "Epoch 39/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.0606\n",
      "Epoch 40/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0584\n",
      "Epoch 41/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0639\n",
      "Epoch 42/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0550\n",
      "Epoch 43/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0553\n",
      "Epoch 44/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0533\n",
      "Epoch 45/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0534\n",
      "Epoch 46/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0559\n",
      "Epoch 47/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0475\n",
      "Epoch 48/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0480\n",
      "Epoch 49/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0479\n",
      "Epoch 50/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0473\n",
      "Epoch 51/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0473\n",
      "Epoch 52/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0467\n",
      "Epoch 53/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0459\n",
      "Epoch 54/66\n",
      "1801/1801 [==============================] - 39s 21ms/step - loss: 0.0411\n",
      "Epoch 55/66\n",
      "1801/1801 [==============================] - 37s 20ms/step - loss: 0.0418\n",
      "Epoch 56/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0395\n",
      "Epoch 57/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0387\n",
      "Epoch 58/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.0389\n",
      "Epoch 59/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0383\n",
      "Epoch 60/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0379\n",
      "Epoch 61/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0327\n",
      "Epoch 62/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0361\n",
      "Epoch 63/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0335\n",
      "Epoch 64/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0354\n",
      "Epoch 65/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0333\n",
      "Epoch 66/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0305\n",
      "451/451 [==============================] - 2s 4ms/step\n",
      "R2 Score on Test Data: 0.7450759202776083\n",
      "Mean Squared Error: 40729936086.218155\n",
      "Root Mean Squared Error: 201816.59021551762\n",
      "Mean Absolute Error: 105075.2487312756\n",
      "Mean Absolute percentage Error: 18.79169932356359\n",
      "explained variance 0.7450835852255022\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_model(x_train, y_train, x_test, y_test, params):\n",
    "    \"\"\"\n",
    "    Train and evaluate a neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train: Training features\n",
    "    - y_train: Training target values\n",
    "    - x_test: Test features\n",
    "    - y_test: Test target values\n",
    "    - params: Dictionary containing the optimal parameters\n",
    "\n",
    "    Returns:\n",
    "    - R2 score, MSE, and RMSE on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    if params['regularization'] == 'l1':\n",
    "        reg = l1(params['l1_reg'])\n",
    "    elif params['regularization'] == 'l2':\n",
    "        reg = l2(0)  \n",
    "    elif params['regularization'] == 'l1_l2':\n",
    "        reg = l1_l2(l1=params['l1_reg'], l2=0)  \n",
    "    else:\n",
    "        reg = None\n",
    "\n",
    "    model.add(Dense(params['neurons_layer_1'], activation='relu', input_shape=(x_train.shape[1],), kernel_regularizer=reg))\n",
    "    model.add(Dense(params['neurons_layer_2'], activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dense(params['neurons_layer_3'], activation='relu', kernel_regularizer=reg))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Scaling y_train\n",
    "    scaler = StandardScaler()\n",
    "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train_scaled, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
    "\n",
    "    # Predict and evaluate on the test set\n",
    "    y_pred_scaled = model.predict(x_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "     # Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mse)\n",
    "    # R-squared Score\n",
    "    r2_score_value = r2_score(y_test, y_pred)\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    # Explained Variance Score\n",
    "    explained_variance = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    return r2_score_value, mse, rmse, mae, mape, explained_variance\n",
    "\n",
    "\n",
    "optimal_params = {'learning_rate': 0.00014245324321009346, 'neurons_layer_1': 1008, 'neurons_layer_2': 288, 'neurons_layer_3': 240, 'batch_size': 32, 'epochs': 66, 'regularization': 'none'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r2_score_value, mse, rmse, mae, mape, explained_variance = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
    "print(\"R2 Score on Test Data:\", r2_score_value)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute percentage Error:\", mape)\n",
    "print(\"explained variance\", explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### final model NN TEXT ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\df-dutch.csv\")\n",
    "\n",
    "\n",
    "df[\"ada_embedding\"] = df[\"ada_embedding\"].apply(literal_eval).apply(np.array)\n",
    "\n",
    "x = df[\"ada_embedding\"]\n",
    "y = df['numerical_price']\n",
    "\n",
    "del df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(x), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (57611, 1536)\n",
      "x_test shape: (14403, 1536)\n",
      "y_train shape: (57611,)\n",
      "y_test shape: (14403,)\n",
      "Adjusted y_train shape: (57611,)\n",
      "Adjusted y_test shape: (14403,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "print(\"Adjusted y_train shape:\", y_train.shape)\n",
    "print(\"Adjusted y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.5117\n",
      "Epoch 2/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.4017\n",
      "Epoch 3/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.3643\n",
      "Epoch 4/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.3446\n",
      "Epoch 5/66\n",
      "1801/1801 [==============================] - 32s 17ms/step - loss: 0.3068\n",
      "Epoch 6/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.2841\n",
      "Epoch 7/66\n",
      "1801/1801 [==============================] - 37s 21ms/step - loss: 0.2707\n",
      "Epoch 8/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2481\n",
      "Epoch 9/66\n",
      "1801/1801 [==============================] - 38s 21ms/step - loss: 0.2340\n",
      "Epoch 10/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.2317\n",
      "Epoch 11/66\n",
      "1801/1801 [==============================] - 36s 20ms/step - loss: 0.2126\n",
      "Epoch 12/66\n",
      "1801/1801 [==============================] - 36s 20ms/step - loss: 0.2011\n",
      "Epoch 13/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1875\n",
      "Epoch 14/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.1917\n",
      "Epoch 15/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.1693\n",
      "Epoch 16/66\n",
      "1801/1801 [==============================] - 37s 20ms/step - loss: 0.1617\n",
      "Epoch 17/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1615\n",
      "Epoch 18/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.1541\n",
      "Epoch 19/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1502\n",
      "Epoch 20/66\n",
      "1801/1801 [==============================] - 38s 21ms/step - loss: 0.1413\n",
      "Epoch 21/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1403\n",
      "Epoch 22/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.1368\n",
      "Epoch 23/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1255\n",
      "Epoch 24/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1168\n",
      "Epoch 25/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.1164\n",
      "Epoch 26/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1106\n",
      "Epoch 27/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.1054\n",
      "Epoch 28/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1077\n",
      "Epoch 29/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1039\n",
      "Epoch 30/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0986\n",
      "Epoch 31/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0971\n",
      "Epoch 32/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0903\n",
      "Epoch 33/66\n",
      "1801/1801 [==============================] - 38s 21ms/step - loss: 0.0844\n",
      "Epoch 34/66\n",
      "1801/1801 [==============================] - 37s 20ms/step - loss: 0.0849\n",
      "Epoch 35/66\n",
      "1801/1801 [==============================] - 36s 20ms/step - loss: 0.0843\n",
      "Epoch 36/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0831\n",
      "Epoch 37/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0760\n",
      "Epoch 38/66\n",
      "1801/1801 [==============================] - 36s 20ms/step - loss: 0.0750\n",
      "Epoch 39/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0778\n",
      "Epoch 40/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0729\n",
      "Epoch 41/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0691\n",
      "Epoch 42/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0666\n",
      "Epoch 43/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0655\n",
      "Epoch 44/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0640\n",
      "Epoch 45/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0592\n",
      "Epoch 46/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0581\n",
      "Epoch 47/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0614\n",
      "Epoch 48/66\n",
      "1801/1801 [==============================] - 37s 21ms/step - loss: 0.0576\n",
      "Epoch 49/66\n",
      "1801/1801 [==============================] - 37s 21ms/step - loss: 0.0569\n",
      "Epoch 50/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0524\n",
      "Epoch 51/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0519\n",
      "Epoch 52/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0534\n",
      "Epoch 53/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0520\n",
      "Epoch 54/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0462\n",
      "Epoch 55/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0513\n",
      "Epoch 56/66\n",
      "1801/1801 [==============================] - 36s 20ms/step - loss: 0.0477\n",
      "Epoch 57/66\n",
      "1801/1801 [==============================] - 35s 19ms/step - loss: 0.0470\n",
      "Epoch 58/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0470\n",
      "Epoch 59/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0440\n",
      "Epoch 60/66\n",
      "1801/1801 [==============================] - 33s 19ms/step - loss: 0.0420\n",
      "Epoch 61/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.0457\n",
      "Epoch 62/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.0434\n",
      "Epoch 63/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.0409\n",
      "Epoch 64/66\n",
      "1801/1801 [==============================] - 30s 16ms/step - loss: 0.0392\n",
      "Epoch 65/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.0394\n",
      "Epoch 66/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0399\n",
      "451/451 [==============================] - 2s 4ms/step\n",
      "R2 Score on Test Data: 0.7204084314694288\n",
      "Mean Squared Error: 44671130043.48871\n",
      "Root Mean Squared Error: 211355.45898672385\n",
      "Mean Absolute Error: 106848.18583232442\n",
      "Mean Absolute percentage Error: 18.410169969405764\n",
      "explained variance 0.7220819718713023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "r2_score_value, mse, rmse, mae, mape, explained_variance = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
    "print(\"R2 Score on Test Data:\", r2_score_value)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute percentage Error:\", mape)\n",
    "print(\"explained variance\", explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### final model NN TEXT ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\df-prompt-1.csv\")\n",
    "\n",
    "nan_count = df['ada_embedding_prompt'].isna().sum()\n",
    "\n",
    "df = df.dropna(subset=['ada_embedding_prompt'])\n",
    "\n",
    "\n",
    "\n",
    "df[\"ada_embedding_prompt\"] = df[\"ada_embedding_prompt\"].apply(literal_eval).apply(np.array)\n",
    "\n",
    "x = df[\"ada_embedding_prompt\"]\n",
    "y = df['numerical_price']\n",
    "\n",
    "del df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(x), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (57595, 1536)\n",
      "x_test shape: (14399, 1536)\n",
      "y_train shape: (57595,)\n",
      "y_test shape: (14399,)\n",
      "Adjusted y_train shape: (57595,)\n",
      "Adjusted y_test shape: (14399,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "print(\"Adjusted y_train shape:\", y_train.shape)\n",
    "print(\"Adjusted y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "1800/1800 [==============================] - 35s 19ms/step - loss: 0.5204\n",
      "Epoch 2/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.4225\n",
      "Epoch 3/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.3728\n",
      "Epoch 4/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.3482\n",
      "Epoch 5/66\n",
      "1800/1800 [==============================] - 33s 18ms/step - loss: 0.3152\n",
      "Epoch 6/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2882\n",
      "Epoch 7/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2649\n",
      "Epoch 8/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2427\n",
      "Epoch 9/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2295\n",
      "Epoch 10/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2245\n",
      "Epoch 11/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2122\n",
      "Epoch 12/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.2043\n",
      "Epoch 13/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1888\n",
      "Epoch 14/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1866\n",
      "Epoch 15/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1678\n",
      "Epoch 16/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1618\n",
      "Epoch 17/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1577\n",
      "Epoch 18/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1501\n",
      "Epoch 19/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1459\n",
      "Epoch 20/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1405\n",
      "Epoch 21/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1340\n",
      "Epoch 22/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1291\n",
      "Epoch 23/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1214\n",
      "Epoch 24/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1195\n",
      "Epoch 25/66\n",
      "1800/1800 [==============================] - 31s 17ms/step - loss: 0.1181\n",
      "Epoch 26/66\n",
      "1800/1800 [==============================] - 35s 19ms/step - loss: 0.1118\n",
      "Epoch 27/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.1066\n",
      "Epoch 28/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0987\n",
      "Epoch 29/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.0994\n",
      "Epoch 30/66\n",
      "1800/1800 [==============================] - 33s 18ms/step - loss: 0.0931\n",
      "Epoch 31/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.0997\n",
      "Epoch 32/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.0896\n",
      "Epoch 33/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.0830\n",
      "Epoch 34/66\n",
      "1800/1800 [==============================] - 38s 21ms/step - loss: 0.0859\n",
      "Epoch 35/66\n",
      "1800/1800 [==============================] - 39s 21ms/step - loss: 0.0841\n",
      "Epoch 36/66\n",
      "1800/1800 [==============================] - 37s 21ms/step - loss: 0.0751\n",
      "Epoch 37/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0737\n",
      "Epoch 38/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0738\n",
      "Epoch 39/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0763\n",
      "Epoch 40/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0658\n",
      "Epoch 41/66\n",
      "1800/1800 [==============================] - 37s 20ms/step - loss: 0.0637\n",
      "Epoch 42/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0606\n",
      "Epoch 43/66\n",
      "1800/1800 [==============================] - 37s 20ms/step - loss: 0.0664\n",
      "Epoch 44/66\n",
      "1800/1800 [==============================] - 37s 20ms/step - loss: 0.0596\n",
      "Epoch 45/66\n",
      "1800/1800 [==============================] - 37s 20ms/step - loss: 0.0574\n",
      "Epoch 46/66\n",
      "1800/1800 [==============================] - 37s 21ms/step - loss: 0.0635\n",
      "Epoch 47/66\n",
      "1800/1800 [==============================] - 37s 20ms/step - loss: 0.0622\n",
      "Epoch 48/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0552\n",
      "Epoch 49/66\n",
      "1800/1800 [==============================] - 34s 19ms/step - loss: 0.0538\n",
      "Epoch 50/66\n",
      "1800/1800 [==============================] - 33s 18ms/step - loss: 0.0501\n",
      "Epoch 51/66\n",
      "1800/1800 [==============================] - 33s 18ms/step - loss: 0.0472\n",
      "Epoch 52/66\n",
      "1800/1800 [==============================] - 33s 18ms/step - loss: 0.0477\n",
      "Epoch 53/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0482\n",
      "Epoch 54/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0461\n",
      "Epoch 55/66\n",
      "1800/1800 [==============================] - 35s 19ms/step - loss: 0.0476\n",
      "Epoch 56/66\n",
      "1800/1800 [==============================] - 35s 19ms/step - loss: 0.0424\n",
      "Epoch 57/66\n",
      "1800/1800 [==============================] - 37s 21ms/step - loss: 0.0469\n",
      "Epoch 58/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0423\n",
      "Epoch 59/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0428\n",
      "Epoch 60/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0424\n",
      "Epoch 61/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0355\n",
      "Epoch 62/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0421\n",
      "Epoch 63/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0394\n",
      "Epoch 64/66\n",
      "1800/1800 [==============================] - 35s 20ms/step - loss: 0.0386\n",
      "Epoch 65/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0383\n",
      "Epoch 66/66\n",
      "1800/1800 [==============================] - 36s 20ms/step - loss: 0.0355\n",
      "450/450 [==============================] - 2s 4ms/step\n",
      "R2 Score on Test Data: 0.7022890405088038\n",
      "Mean Squared Error: 46133124282.186195\n",
      "Root Mean Squared Error: 214786.22926571945\n",
      "Mean Absolute Error: 108385.40808253828\n",
      "Mean Absolute percentage Error: 18.235359837323834\n",
      "explained variance 0.7037632979195078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r2_score_value, mse, rmse, mae, mape, explained_variance = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
    "print(\"R2 Score on Test Data:\", r2_score_value)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute percentage Error:\", mape)\n",
    "print(\"explained variance\", explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### final model NN TEXT ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\df-anti1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"description_en_anti_prompt_embedding\"] = df[\"description_en_anti_prompt_embedding\"].apply(literal_eval).apply(np.array)\n",
    "\n",
    "x = df[\"description_en_anti_prompt_embedding\"]\n",
    "y = df['numerical_price']\n",
    "\n",
    "del df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(x), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (57611, 1536)\n",
      "x_test shape: (14403, 1536)\n",
      "y_train shape: (57611,)\n",
      "y_test shape: (14403,)\n",
      "Adjusted y_train shape: (57611,)\n",
      "Adjusted y_test shape: (14403,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "print(\"Adjusted y_train shape:\", y_train.shape)\n",
    "print(\"Adjusted y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.5406\n",
      "Epoch 2/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.4336\n",
      "Epoch 3/66\n",
      "1801/1801 [==============================] - 30s 17ms/step - loss: 0.3917\n",
      "Epoch 4/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.3607\n",
      "Epoch 5/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.3336\n",
      "Epoch 6/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.3097\n",
      "Epoch 7/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2839\n",
      "Epoch 8/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2600\n",
      "Epoch 9/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2419\n",
      "Epoch 10/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2262\n",
      "Epoch 11/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2114\n",
      "Epoch 12/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.2114\n",
      "Epoch 13/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1938\n",
      "Epoch 14/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1877\n",
      "Epoch 15/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1744\n",
      "Epoch 16/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1731\n",
      "Epoch 17/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1630\n",
      "Epoch 18/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1582\n",
      "Epoch 19/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1489\n",
      "Epoch 20/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1397\n",
      "Epoch 21/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1472\n",
      "Epoch 22/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1436\n",
      "Epoch 23/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1245\n",
      "Epoch 24/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1207\n",
      "Epoch 25/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1190\n",
      "Epoch 26/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1204\n",
      "Epoch 27/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1062\n",
      "Epoch 28/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1124\n",
      "Epoch 29/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1005\n",
      "Epoch 30/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0992\n",
      "Epoch 31/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0984\n",
      "Epoch 32/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0907\n",
      "Epoch 33/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0937\n",
      "Epoch 34/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0891\n",
      "Epoch 35/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0845\n",
      "Epoch 36/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0836\n",
      "Epoch 37/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0842\n",
      "Epoch 38/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0780\n",
      "Epoch 39/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0755\n",
      "Epoch 40/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0730\n",
      "Epoch 41/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0728\n",
      "Epoch 42/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0689\n",
      "Epoch 43/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0651\n",
      "Epoch 44/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0646\n",
      "Epoch 45/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0694\n",
      "Epoch 46/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0567\n",
      "Epoch 47/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0600\n",
      "Epoch 48/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0574\n",
      "Epoch 49/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0562\n",
      "Epoch 50/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0547\n",
      "Epoch 51/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0551\n",
      "Epoch 52/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0586\n",
      "Epoch 53/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0522\n",
      "Epoch 54/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0516\n",
      "Epoch 55/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0499\n",
      "Epoch 56/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0479\n",
      "Epoch 57/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0507\n",
      "Epoch 58/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0438\n",
      "Epoch 59/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0467\n",
      "Epoch 60/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0450\n",
      "Epoch 61/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0417\n",
      "Epoch 62/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0424\n",
      "Epoch 63/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0453\n",
      "Epoch 64/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0434\n",
      "Epoch 65/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0403\n",
      "Epoch 66/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0381\n",
      "451/451 [==============================] - 2s 4ms/step\n",
      "R2 Score on Test Data: 0.7171933312241126\n",
      "Mean Squared Error: 45184815638.21605\n",
      "Root Mean Squared Error: 212567.20264004992\n",
      "Mean Absolute Error: 108280.76617501563\n",
      "Mean Absolute percentage Error: 18.788940313508142\n",
      "explained variance 0.718286415709118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r2_score_value, mse, rmse, mae, mape, explained_variance = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
    "print(\"R2 Score on Test Data:\", r2_score_value)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute percentage Error:\", mape)\n",
    "print(\"explained variance\", explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\f-string-embedding.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"f_string_embedding\"] = df[\"f_string_embedding\"].apply(literal_eval).apply(np.array)\n",
    "\n",
    "x = df[\"f_string_embedding\"]\n",
    "y = df['numerical_price']\n",
    "\n",
    "del df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list(x), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (57610, 1536)\n",
      "x_test shape: (14403, 1536)\n",
      "y_train shape: (57610,)\n",
      "y_test shape: (14403,)\n",
      "Adjusted y_train shape: (57610,)\n",
      "Adjusted y_test shape: (14403,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "\n",
    "print(\"Adjusted y_train shape:\", y_train.shape)\n",
    "print(\"Adjusted y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.4821\n",
      "Epoch 2/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.3405\n",
      "Epoch 3/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.3013\n",
      "Epoch 4/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2699\n",
      "Epoch 5/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2408\n",
      "Epoch 6/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.2166\n",
      "Epoch 7/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1952\n",
      "Epoch 8/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1944\n",
      "Epoch 9/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1692\n",
      "Epoch 10/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1734\n",
      "Epoch 11/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1473\n",
      "Epoch 12/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1452\n",
      "Epoch 13/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1451\n",
      "Epoch 14/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1267\n",
      "Epoch 15/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1197\n",
      "Epoch 16/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1202\n",
      "Epoch 17/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.1167\n",
      "Epoch 18/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.1100\n",
      "Epoch 19/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0995\n",
      "Epoch 20/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0971\n",
      "Epoch 21/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0904\n",
      "Epoch 22/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0902\n",
      "Epoch 23/66\n",
      "1801/1801 [==============================] - 37s 20ms/step - loss: 0.0933\n",
      "Epoch 24/66\n",
      "1801/1801 [==============================] - 34s 19ms/step - loss: 0.0843\n",
      "Epoch 25/66\n",
      "1801/1801 [==============================] - 35s 20ms/step - loss: 0.0835\n",
      "Epoch 26/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0783\n",
      "Epoch 27/66\n",
      "1801/1801 [==============================] - 31s 17ms/step - loss: 0.0750\n",
      "Epoch 28/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0714\n",
      "Epoch 29/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0705\n",
      "Epoch 30/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0629\n",
      "Epoch 31/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0655\n",
      "Epoch 32/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0677\n",
      "Epoch 33/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0586\n",
      "Epoch 34/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0560\n",
      "Epoch 35/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0575\n",
      "Epoch 36/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0540\n",
      "Epoch 37/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0516\n",
      "Epoch 38/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0545\n",
      "Epoch 39/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0476\n",
      "Epoch 40/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0454\n",
      "Epoch 41/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0451\n",
      "Epoch 42/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0468\n",
      "Epoch 43/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0406\n",
      "Epoch 44/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0401\n",
      "Epoch 45/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0450\n",
      "Epoch 46/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0379\n",
      "Epoch 47/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0357\n",
      "Epoch 48/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0401\n",
      "Epoch 49/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0338\n",
      "Epoch 50/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0359\n",
      "Epoch 51/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0339\n",
      "Epoch 52/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0340\n",
      "Epoch 53/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0296\n",
      "Epoch 54/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0338\n",
      "Epoch 55/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0308\n",
      "Epoch 56/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0301\n",
      "Epoch 57/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0283\n",
      "Epoch 58/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0316\n",
      "Epoch 59/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0293\n",
      "Epoch 60/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0233\n",
      "Epoch 61/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0245\n",
      "Epoch 62/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0265\n",
      "Epoch 63/66\n",
      "1801/1801 [==============================] - 32s 18ms/step - loss: 0.0245\n",
      "Epoch 64/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0245\n",
      "Epoch 65/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0272\n",
      "Epoch 66/66\n",
      "1801/1801 [==============================] - 33s 18ms/step - loss: 0.0240\n",
      "451/451 [==============================] - 2s 3ms/step\n",
      "R2 Score on Test Data: 0.7925505495106119\n",
      "Mean Squared Error: 33380739958.57869\n",
      "Root Mean Squared Error: 182703.9680975175\n",
      "Mean Absolute Error: 84249.71212323387\n",
      "Mean Absolute percentage Error: 14.264152817153345\n",
      "explained variance 0.7930492683971089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "r2_score_value, mse, rmse, mae, mape, explained_variance = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
    "print(\"R2 Score on Test Data:\", r2_score_value)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Absolute percentage Error:\", mape)\n",
    "print(\"explained variance\", explained_variance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
