{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIXDbvlA0b3",
        "outputId": "72cb3dfa-00c4-4481-8d73-16fed5a2a0ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['numerical_price',\n",
              " 'embeddings',\n",
              " 'num_bedrooms',\n",
              " 'num_rooms',\n",
              " 'building_type_Bestaande bouw',\n",
              " 'building_type_Nieuwbouw',\n",
              " 'building_type_na',\n",
              " 'tag_k.k.',\n",
              " 'tag_v.o.n.',\n",
              " 'house_category_Appartement',\n",
              " 'house_category_Bungalow',\n",
              " 'house_category_Eengezinswoning',\n",
              " 'house_category_Grachtenpand',\n",
              " 'house_category_Herenhuis',\n",
              " 'house_category_Landhuis',\n",
              " 'house_category_Other',\n",
              " 'house_category_Unknown',\n",
              " 'house_category_Villa',\n",
              " 'house_category_Woonboerderij',\n",
              " 'house_category_Woonboot',\n",
              " 'energy_label_encoded',\n",
              " 'size_scaled',\n",
              " 'longitude_scaled',\n",
              " 'latitude_scaled']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# This will prompt for authorization to access your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the file path to the location where you uploaded your CSV in Google Drive\n",
        "file_path = '/content/drive/My Drive/Thesis/df-BERT_cat.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop the first column (if it's an unwanted index column)\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "\n",
        "# Display the column names\n",
        "df.columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "\n",
        "df[\"embeddings\"] = df[\"embeddings\"].apply(literal_eval).apply(np.array)\n",
        "\n",
        "# Separating the target variable and embeddings\n",
        "x = df[\"embeddings\"]\n",
        "y = df['numerical_price']\n",
        "\n",
        "# Dropping the 'ada_embedding_eng' column from df\n",
        "z = df.drop([\"embeddings\", \"numerical_price\"], axis=1)\n",
        "\n",
        "\n",
        "# Deleting original dataframe for memory purpose\n",
        "del df\n",
        "\n",
        "# Explode the embedding arrays into separate columns\n",
        "x = x.apply(pd.Series)\n",
        "\n",
        "# Concatenating the exploded embeddings with the rest of the data\n",
        "concatenated_df = pd.concat([x, z], axis=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "yYHgwEXkBAzX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "concatenated_df.columns = concatenated_df.columns.astype(str)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(concatenated_df, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "uPpnaJFjBDEG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate(x_train, y_train, x_test, y_test, best_params):\n",
        "    \"\"\"\n",
        "    Trains the Random Forest model with the given parameters and evaluates it on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (list/array): Training features\n",
        "    y_train (list/array): Training target variable\n",
        "    x_test (list/array): Test features\n",
        "    y_test (list/array): Test target variable\n",
        "    best_params (dict): Dictionary of best hyperparameters\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing R2, MSE, and RMSE metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize RandomForestRegressor with best parameters\n",
        "    clf_rf_best = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Initialize and fit StandardScaler on y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "\n",
        "    # Train the model on the entire training dataset\n",
        "    clf_rf_best.fit(x_train, y_train_scaled.ravel())\n",
        "\n",
        "    # Predict on the test data and inverse transform the predictions\n",
        "    y_pred_scaled = clf_rf_best.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "best_params = {'max_depth': 24, 'n_estimators': 550, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 8, 'bootstrap': False}\n",
        "performance_metrics = train_and_evaluate(x_train, y_train, x_test, y_test, best_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID4JOjD7BiNg",
        "outputId": "1efb4478-109c-4435-8254-5b093b28392e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance on Test Set: {'R2': 0.5795396175647093, 'MSE': 67656379208.75064, 'RMSE': 260108.39895849314, 'MAE': 125823.94117539897, 'MAPE': 25.48885406788416, 'explained': 0.5798537296891237}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def run_svr_and_evaluate(x_train,x_test, y_train,y_test ):\n",
        "    # Split data into training and testing sets\n",
        "\n",
        "\n",
        "    # Initialize SVR with your predefined parameters\n",
        "    clf_svr_opt =  LinearSVR(**linear_svr_params, random_state=0)\n",
        "\n",
        "    # StandardScaler for y\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Scaling\n",
        "    y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "\n",
        "    # Fitting the model\n",
        "    clf_svr_opt.fit(x_train, y_train_scaled)\n",
        "\n",
        "    # Predicting and inverse transformation for the test set\n",
        "    y_pred_scaled = clf_svr_opt.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "linear_svr_params = {'C': 0.22348106317685834, 'epsilon': 0.0010146073912052189, 'tol': 0.003974565015315399, 'loss': 'squared_epsilon_insensitive', 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1.560470019225011, 'max_iter': 8343}\n",
        "\n",
        "performance_metrics = run_svr_and_evaluate(x_train=x_train,x_test=x_test, y_train=y_train,y_test=y_test)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRwmHj6aBwSR",
        "outputId": "ff9b2a58-e75a-4bf5-dc52-ff320752d93a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance on Test Set: {'R2': 0.6420403839908111, 'MSE': 57599366156.367096, 'RMSE': 239998.67948879863, 'MAE': 133614.8637361109, 'MAPE': 25.950019264182412, 'explained': 0.6423670018064684}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate_model(x_train, y_train, x_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Train and evaluate a neural network model.\n",
        "\n",
        "    Parameters:\n",
        "    - x_train: Training features\n",
        "    - y_train: Training target values\n",
        "    - x_test: Test features\n",
        "    - y_test: Test target values\n",
        "    - params: Dictionary containing the optimal parameters\n",
        "\n",
        "    Returns:\n",
        "    - R2 score, MSE, and RMSE on the test set.\n",
        "    \"\"\"\n",
        "    x_train = x_train.to_numpy()\n",
        "    x_test = x_test.to_numpy()\n",
        "    y_train = y_train.to_numpy()\n",
        "    y_test = y_test.to_numpy()\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    y_train = y_train.astype('float32')\n",
        "    y_test = y_test.astype('float32')\n",
        "\n",
        "\n",
        "    y_train = np.ravel(y_train)\n",
        "    y_test = np.ravel(y_test)\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    if params['regularization'] == 'l1':\n",
        "        reg = l1(params['l1_reg'])\n",
        "    elif params['regularization'] == 'l2':\n",
        "        reg = l2(0)\n",
        "    elif params['regularization'] == 'l1_l2':\n",
        "        reg = l1_l2(l1=params['l1_reg'], l2=0)\n",
        "    else:\n",
        "        reg = None\n",
        "\n",
        "    model.add(Dense(params['neurons_layer_1'], activation='relu', input_shape=(x_train.shape[1],), kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_2'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_3'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Scaling y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train_scaled, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
        "\n",
        "    # Predict and evaluate on the test set\n",
        "    y_pred_scaled = model.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "optimal_params = {'learning_rate': 0.00011723245844623722, 'neurons_layer_1': 624, 'neurons_layer_2': 32, 'neurons_layer_3': 256, 'batch_size': 64, 'epochs': 78, 'regularization': 'l1', 'l1_reg': 1.802535830576335e-05}\n",
        "\n",
        "performance_metrics = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WdXoWg5B7h_",
        "outputId": "31379e9e-6e2c-4ded-8b29-11142108b17f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/78\n",
            "901/901 [==============================] - 12s 12ms/step - loss: 0.7179\n",
            "Epoch 2/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.5637\n",
            "Epoch 3/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.5155\n",
            "Epoch 4/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.4847\n",
            "Epoch 5/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.4672\n",
            "Epoch 6/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.4467\n",
            "Epoch 7/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.4375\n",
            "Epoch 8/78\n",
            "901/901 [==============================] - 10s 12ms/step - loss: 0.4243\n",
            "Epoch 9/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.4220\n",
            "Epoch 10/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.4106\n",
            "Epoch 11/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.4013\n",
            "Epoch 12/78\n",
            "901/901 [==============================] - 11s 13ms/step - loss: 0.3865\n",
            "Epoch 13/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3808\n",
            "Epoch 14/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3761\n",
            "Epoch 15/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.3651\n",
            "Epoch 16/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.3694\n",
            "Epoch 17/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3595\n",
            "Epoch 18/78\n",
            "901/901 [==============================] - 10s 12ms/step - loss: 0.3501\n",
            "Epoch 19/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3568\n",
            "Epoch 20/78\n",
            "901/901 [==============================] - 12s 14ms/step - loss: 0.3336\n",
            "Epoch 21/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.3388\n",
            "Epoch 22/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.3296\n",
            "Epoch 23/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3219\n",
            "Epoch 24/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3174\n",
            "Epoch 25/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.3218\n",
            "Epoch 26/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3456\n",
            "Epoch 27/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3042\n",
            "Epoch 28/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.3038\n",
            "Epoch 29/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.3059\n",
            "Epoch 30/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2910\n",
            "Epoch 31/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.3058\n",
            "Epoch 32/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2915\n",
            "Epoch 33/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2894\n",
            "Epoch 34/78\n",
            "901/901 [==============================] - 17s 19ms/step - loss: 0.2840\n",
            "Epoch 35/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2784\n",
            "Epoch 36/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2786\n",
            "Epoch 37/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2784\n",
            "Epoch 38/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2804\n",
            "Epoch 39/78\n",
            "901/901 [==============================] - 11s 13ms/step - loss: 0.2808\n",
            "Epoch 40/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2642\n",
            "Epoch 41/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2712\n",
            "Epoch 42/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2681\n",
            "Epoch 43/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2526\n",
            "Epoch 44/78\n",
            "901/901 [==============================] - 11s 13ms/step - loss: 0.2595\n",
            "Epoch 45/78\n",
            "901/901 [==============================] - 10s 12ms/step - loss: 0.2723\n",
            "Epoch 46/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2660\n",
            "Epoch 47/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2534\n",
            "Epoch 48/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2544\n",
            "Epoch 49/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2440\n",
            "Epoch 50/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2502\n",
            "Epoch 51/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2426\n",
            "Epoch 52/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2473\n",
            "Epoch 53/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2402\n",
            "Epoch 54/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2386\n",
            "Epoch 55/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2376\n",
            "Epoch 56/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2347\n",
            "Epoch 57/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2411\n",
            "Epoch 58/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2356\n",
            "Epoch 59/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2364\n",
            "Epoch 60/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2256\n",
            "Epoch 61/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2296\n",
            "Epoch 62/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2211\n",
            "Epoch 63/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2235\n",
            "Epoch 64/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2291\n",
            "Epoch 65/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2262\n",
            "Epoch 66/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2301\n",
            "Epoch 67/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2208\n",
            "Epoch 68/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2195\n",
            "Epoch 69/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2213\n",
            "Epoch 70/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2120\n",
            "Epoch 71/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2154\n",
            "Epoch 72/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2230\n",
            "Epoch 73/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2194\n",
            "Epoch 74/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2163\n",
            "Epoch 75/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2153\n",
            "Epoch 76/78\n",
            "901/901 [==============================] - 12s 13ms/step - loss: 0.2046\n",
            "Epoch 77/78\n",
            "901/901 [==============================] - 10s 11ms/step - loss: 0.2057\n",
            "Epoch 78/78\n",
            "901/901 [==============================] - 11s 12ms/step - loss: 0.2077\n",
            "451/451 [==============================] - 1s 3ms/step\n",
            "Final Model Performance on Test Set: {'R2': 0.7787071716354658, 'MSE': 35608280000.0, 'RMSE': 188701.56, 'MAE': 100810.11, 'MAPE': 17.752599716186523, 'explained': 0.7787110805511475}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KwFs5RnBrUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}