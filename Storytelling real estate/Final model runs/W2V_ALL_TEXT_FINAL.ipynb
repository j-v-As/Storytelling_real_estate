{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCIJMcAQ_D-H",
        "outputId": "6752d6b4-c6f4-4d61-c628-a55c13433285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url',\n",
              " 'price',\n",
              " 'address',\n",
              " 'descrip',\n",
              " 'listed_since',\n",
              " 'zip_code',\n",
              " 'size',\n",
              " 'year',\n",
              " 'living_area',\n",
              " 'kind_of_house',\n",
              " 'building_type',\n",
              " 'num_of_rooms',\n",
              " 'num_of_bathrooms',\n",
              " 'layout',\n",
              " 'energy_label',\n",
              " 'insulation',\n",
              " 'heating',\n",
              " 'ownership',\n",
              " 'exteriors',\n",
              " 'parking',\n",
              " 'date_list',\n",
              " 'last_ask_price',\n",
              " 'last_ask_price_m2',\n",
              " 'city',\n",
              " 'log_id',\n",
              " 'num of tokens per descrip',\n",
              " 'descrip_en',\n",
              " 'numerical_price',\n",
              " 'numerical_price_per_m2',\n",
              " 'tag',\n",
              " 'house_category',\n",
              " 'living_area_float',\n",
              " 'size_float',\n",
              " 'zip_code_4_digits',\n",
              " 'postcode',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'processed_descrip',\n",
              " 'word2vec_embeddings']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# This will prompt for authorization to access your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the file path to the location where you uploaded your CSV in Google Drive\n",
        "file_path = '/content/drive/My Drive/Thesis/df-W2V.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop the first column (if it's an unwanted index column)\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "\n",
        "# Display the column names\n",
        "df.columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "\n",
        "df[\"word2vec_embeddings\"] = df[\"word2vec_embeddings\"].apply(literal_eval).apply(np.array)\n",
        "\n",
        "x = df[\"word2vec_embeddings\"]\n",
        "y = df['numerical_price']\n",
        "# Explode the embedding arrays into separate columns\n",
        "x = x.apply(pd.Series)\n",
        "\n",
        "# First split: separate out a test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "RBYrP2xc_P6i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate(x_train, y_train, x_test, y_test, best_params):\n",
        "    \"\"\"\n",
        "    Trains the Random Forest model with the given parameters and evaluates it on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (list/array): Training features\n",
        "    y_train (list/array): Training target variable\n",
        "    x_test (list/array): Test features\n",
        "    y_test (list/array): Test target variable\n",
        "    best_params (dict): Dictionary of best hyperparameters\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing R2, MSE, and RMSE metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize RandomForestRegressor with best parameters\n",
        "    clf_rf_best = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Initialize and fit StandardScaler on y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "\n",
        "    # Train the model on the entire training dataset\n",
        "    clf_rf_best.fit(x_train, y_train_scaled.ravel())\n",
        "\n",
        "    # Predict on the test data and inverse transform the predictions\n",
        "    y_pred_scaled = clf_rf_best.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "# Best parameters from cross-validation\n",
        "best_params = {'max_depth': 50, 'n_estimators': 850, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'bootstrap': False}\n",
        "\n",
        "\n",
        "\n",
        "performance_metrics = train_and_evaluate(x_train, y_train, x_test, y_test, best_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urZPl4Jm_oaN",
        "outputId": "8483dcf6-87cb-4449-b5cf-072bc953c27c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance on Test Set: {'R2': 0.4260369795422405, 'MSE': 91703683561.6116, 'RMSE': 302826.160629513, 'MAE': 156614.54219802687, 'MAPE': 32.64453983247673, 'explained': 0.4266177572084574}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def run_svr_and_evaluate(x_train,x_test, y_train,y_test, ):\n",
        "    # Split data into training and testing sets\n",
        "\n",
        "\n",
        "    # Initialize SVR with your predefined parameters\n",
        "    clf_svr_opt = clf_linear_svr = LinearSVR(**linear_svr_params, random_state=0)\n",
        "\n",
        "    # StandardScaler for y\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Scaling\n",
        "    y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "\n",
        "    # Fitting the model\n",
        "    clf_svr_opt.fit(x_train, y_train_scaled)\n",
        "\n",
        "    # Predicting and inverse transformation for the test set\n",
        "    y_pred_scaled = clf_svr_opt.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "\n",
        "linear_svr_params = {'C': 11.189562507362268, 'epsilon': 0.0017852098202047108, 'tol': 2.379529693532015e-05, 'loss': 'squared_epsilon_insensitive', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.2679030967232763, 'max_iter': 7108}\n",
        "\n",
        "performance_metrics = run_svr_and_evaluate(x_train=x_train,x_test=x_test, y_train=y_train,y_test=y_test)\n",
        "print(performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJN8Q74d_3pz",
        "outputId": "e0a81cd1-1914-45a8-a924-1ccd01449660"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'R2': 0.35556921980773293, 'MSE': 102962515419.51617, 'RMSE': 320877.72658680467, 'MAE': 187450.80927310843, 'MAPE': 38.301679362320165, 'explained': 0.35576919410933927}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "y_train = np.ravel(y_train)\n",
        "y_test = np.ravel(y_test)\n",
        "\n",
        "\n",
        "print(\"Adjusted y_train shape:\", y_train.shape)\n",
        "print(\"Adjusted y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD0u6SjxAE6F",
        "outputId": "75e90269-7c7b-49bf-d8f4-a55c242d7502"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (57611, 300)\n",
            "x_test shape: (14403, 300)\n",
            "y_train shape: (57611,)\n",
            "y_test shape: (14403,)\n",
            "Adjusted y_train shape: (57611,)\n",
            "Adjusted y_test shape: (14403,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate_model(x_train, y_train, x_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Train and evaluate a neural network model.\n",
        "\n",
        "    Parameters:\n",
        "    - x_train: Training features\n",
        "    - y_train: Training target values\n",
        "    - x_test: Test features\n",
        "    - y_test: Test target values\n",
        "    - params: Dictionary containing the optimal parameters\n",
        "\n",
        "    Returns:\n",
        "    - R2 score, MSE, and RMSE on the test set.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    if params['regularization'] == 'l1':\n",
        "        reg = l1(params['l1_reg'])\n",
        "    elif params['regularization'] == 'l2':\n",
        "        reg = l2(0)\n",
        "    elif params['regularization'] == 'l1_l2':\n",
        "        reg = l1_l2(l1=params['l1_reg'], l2=0)\n",
        "    else:\n",
        "        reg = None\n",
        "\n",
        "    model.add(Dense(params['neurons_layer_1'], activation='relu', input_shape=(x_train.shape[1],), kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_2'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_3'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Scaling y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train_scaled, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
        "\n",
        "    # Predict and evaluate on the test set\n",
        "    y_pred_scaled = model.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "optimal_params = {'learning_rate': 0.00041361019111463065, 'neurons_layer_1': 832, 'neurons_layer_2': 288, 'neurons_layer_3': 128, 'batch_size': 32, 'epochs': 100, 'regularization': 'none'}\n",
        "\n",
        "\n",
        "\n",
        "performance_metrics = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO1w8baaAKtb",
        "outputId": "c7603db8-c785-4948-e94f-53587d205bf7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.7031\n",
            "Epoch 2/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.5923\n",
            "Epoch 3/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.5689\n",
            "Epoch 4/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.5471\n",
            "Epoch 5/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.5223\n",
            "Epoch 6/100\n",
            "1801/1801 [==============================] - 15s 9ms/step - loss: 0.5092\n",
            "Epoch 7/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.5023\n",
            "Epoch 8/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.4806\n",
            "Epoch 9/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.4662\n",
            "Epoch 10/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.4878\n",
            "Epoch 11/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.4729\n",
            "Epoch 12/100\n",
            "1801/1801 [==============================] - 19s 10ms/step - loss: 0.5710\n",
            "Epoch 13/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4697\n",
            "Epoch 14/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.5870\n",
            "Epoch 15/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.5429\n",
            "Epoch 16/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4806\n",
            "Epoch 17/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4835\n",
            "Epoch 18/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4341\n",
            "Epoch 19/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.4804\n",
            "Epoch 20/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.4493\n",
            "Epoch 21/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.4597\n",
            "Epoch 22/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4365\n",
            "Epoch 23/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4183\n",
            "Epoch 24/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4127\n",
            "Epoch 25/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4231\n",
            "Epoch 26/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.4580\n",
            "Epoch 27/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4009\n",
            "Epoch 28/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.3884\n",
            "Epoch 29/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.3778\n",
            "Epoch 30/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.4011\n",
            "Epoch 31/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.4030\n",
            "Epoch 32/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3895\n",
            "Epoch 33/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3568\n",
            "Epoch 34/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3729\n",
            "Epoch 35/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3531\n",
            "Epoch 36/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3751\n",
            "Epoch 37/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3946\n",
            "Epoch 38/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3442\n",
            "Epoch 39/100\n",
            "1801/1801 [==============================] - 16s 9ms/step - loss: 0.3450\n",
            "Epoch 40/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.3312\n",
            "Epoch 41/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.3454\n",
            "Epoch 42/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3397\n",
            "Epoch 43/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3274\n",
            "Epoch 44/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3311\n",
            "Epoch 45/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3349\n",
            "Epoch 46/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3248\n",
            "Epoch 47/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3133\n",
            "Epoch 48/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3138\n",
            "Epoch 49/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3024\n",
            "Epoch 50/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.3282\n",
            "Epoch 51/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.3241\n",
            "Epoch 52/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3002\n",
            "Epoch 53/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2994\n",
            "Epoch 54/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3040\n",
            "Epoch 55/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2925\n",
            "Epoch 56/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.3051\n",
            "Epoch 57/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2866\n",
            "Epoch 58/100\n",
            "1801/1801 [==============================] - 12s 7ms/step - loss: 0.3101\n",
            "Epoch 59/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2886\n",
            "Epoch 60/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2886\n",
            "Epoch 61/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2929\n",
            "Epoch 62/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2911\n",
            "Epoch 63/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2718\n",
            "Epoch 64/100\n",
            "1801/1801 [==============================] - 12s 7ms/step - loss: 0.2760\n",
            "Epoch 65/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2718\n",
            "Epoch 66/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2700\n",
            "Epoch 67/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2687\n",
            "Epoch 68/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2693\n",
            "Epoch 69/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2561\n",
            "Epoch 70/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2665\n",
            "Epoch 71/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2707\n",
            "Epoch 72/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2585\n",
            "Epoch 73/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2481\n",
            "Epoch 74/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2561\n",
            "Epoch 75/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2528\n",
            "Epoch 76/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2533\n",
            "Epoch 77/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2516\n",
            "Epoch 78/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2465\n",
            "Epoch 79/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2411\n",
            "Epoch 80/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2391\n",
            "Epoch 81/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2532\n",
            "Epoch 82/100\n",
            "1801/1801 [==============================] - 13s 7ms/step - loss: 0.2505\n",
            "Epoch 83/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2334\n",
            "Epoch 84/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2434\n",
            "Epoch 85/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2293\n",
            "Epoch 86/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2326\n",
            "Epoch 87/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2529\n",
            "Epoch 88/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2326\n",
            "Epoch 89/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2249\n",
            "Epoch 90/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2162\n",
            "Epoch 91/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2240\n",
            "Epoch 92/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2240\n",
            "Epoch 93/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2139\n",
            "Epoch 94/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2228\n",
            "Epoch 95/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2144\n",
            "Epoch 96/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2129\n",
            "Epoch 97/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2103\n",
            "Epoch 98/100\n",
            "1801/1801 [==============================] - 14s 8ms/step - loss: 0.2146\n",
            "Epoch 99/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2146\n",
            "Epoch 100/100\n",
            "1801/1801 [==============================] - 15s 8ms/step - loss: 0.2007\n",
            "451/451 [==============================] - 2s 3ms/step\n",
            "Final Model Performance on Test Set: {'R2': 0.6064319756657593, 'MSE': 62881468451.97671, 'RMSE': 250761.77629769794, 'MAE': 137297.42490388287, 'MAPE': 26.140370030585515, 'explained': 0.6066110301949368}\n"
          ]
        }
      ]
    }
  ]
}