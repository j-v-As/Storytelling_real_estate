{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBt1i_W-Dldr",
        "outputId": "cdcb97b3-4632-430d-86b0-6d001b767d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# This will prompt for authorization to access your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the file path to the location where you uploaded your CSV in Google Drive\n",
        "file_path = '/content/drive/My Drive/Thesis/df-W2V_cat.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "df = df.drop(df.columns[0], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df[\"word2vec_embeddings\"] = df[\"word2vec_embeddings\"].apply(literal_eval).apply(np.array)\n",
        "\n",
        "# Separating the target variable and embeddings\n",
        "x = df[\"word2vec_embeddings\"]\n",
        "y = df['numerical_price']\n",
        "\n",
        "# Dropping the 'ada_embedding_eng' column from df\n",
        "z = df.drop([\"word2vec_embeddings\", \"numerical_price\"], axis=1)\n",
        "\n",
        "\n",
        "# Deleting original dataframe for memory purpose\n",
        "del df\n",
        "\n",
        "# Explode the embedding arrays into separate columns\n",
        "x = x.apply(pd.Series)\n",
        "\n",
        "# Concatenating the exploded embeddings with the rest of the data\n",
        "concatenated_df = pd.concat([x, z], axis=1).reset_index(drop=True)\n",
        "concatenated_df.columns = concatenated_df.columns.astype(str)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(concatenated_df, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "huGgYDCdDwG4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate(x_train, y_train, x_test, y_test, best_params):\n",
        "    \"\"\"\n",
        "    Trains the Random Forest model with the given parameters and evaluates it on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (list/array): Training features\n",
        "    y_train (list/array): Training target variable\n",
        "    x_test (list/array): Test features\n",
        "    y_test (list/array): Test target variable\n",
        "    best_params (dict): Dictionary of best hyperparameters\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing R2, MSE, and RMSE metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize RandomForestRegressor with best parameters\n",
        "    clf_rf_best = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Initialize and fit StandardScaler on y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "\n",
        "    # Train the model on the entire training dataset\n",
        "    clf_rf_best.fit(x_train, y_train_scaled.ravel())\n",
        "\n",
        "    # Predict on the test data and inverse transform the predictions\n",
        "    y_pred_scaled = clf_rf_best.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "best_params = {'max_depth': 27, 'n_estimators': 700, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 6, 'bootstrap': False}\n",
        "performance_metrics = train_and_evaluate(x_train, y_train, x_test, y_test, best_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn1b6sJ2D3uu",
        "outputId": "1c3cbf55-a21e-42f0-e6e4-55f4748fdde6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance on Test Set: {'R2': 0.6477441525004195, 'MSE': 56681571421.53406, 'RMSE': 238078.91847354747, 'MAE': 113352.42457478895, 'MAPE': 22.468004739933743, 'explained': 0.6480158818962596}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def run_svr_and_evaluate(x_train,x_test, y_train,y_test ):\n",
        "    # Split data into training and testing sets\n",
        "\n",
        "\n",
        "    # Initialize SVR with your predefined parameters\n",
        "    clf_svr_opt =  LinearSVR(**linear_svr_params, random_state=0)\n",
        "\n",
        "    # StandardScaler for y\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Scaling\n",
        "    y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "\n",
        "    # Fitting the model\n",
        "    clf_svr_opt.fit(x_train, y_train_scaled)\n",
        "\n",
        "    # Predicting and inverse transformation for the test set\n",
        "    y_pred_scaled = clf_svr_opt.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "linear_svr_params = {'C': 5.660841757828869, 'epsilon': 0.0024362459126616902, 'tol': 4.919053826127799e-05, 'loss': 'squared_epsilon_insensitive', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.3420343956050997, 'max_iter': 2899}\n",
        "performance_metrics = run_svr_and_evaluate(x_train=x_train,x_test=x_test, y_train=y_train,y_test=y_test)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvl31LAtED61",
        "outputId": "e1715fca-1890-44a1-95b5-b02d4c42ab17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance on Test Set: {'R2': 0.6287208893024421, 'MSE': 59742609185.081985, 'RMSE': 244423.0127976537, 'MAE': 135480.4040720877, 'MAPE': 25.86734912181677, 'explained': 0.6287319833937672}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate_model(x_train, y_train, x_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Train and evaluate a neural network model.\n",
        "\n",
        "    Parameters:\n",
        "    - x_train: Training features\n",
        "    - y_train: Training target values\n",
        "    - x_test: Test features\n",
        "    - y_test: Test target values\n",
        "    - params: Dictionary containing the optimal parameters\n",
        "\n",
        "    Returns:\n",
        "    - R2 score, MSE, and RMSE on the test set.\n",
        "    \"\"\"\n",
        "    x_train = x_train.to_numpy()\n",
        "    x_test = x_test.to_numpy()\n",
        "    y_train = y_train.to_numpy()\n",
        "    y_test = y_test.to_numpy()\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    y_train = y_train.astype('float32')\n",
        "    y_test = y_test.astype('float32')\n",
        "\n",
        "\n",
        "    y_train = np.ravel(y_train)\n",
        "    y_test = np.ravel(y_test)\n",
        "\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    if params['regularization'] == 'l1':\n",
        "        reg = l1(params['l1_reg'])\n",
        "    elif params['regularization'] == 'l2':\n",
        "        reg = l2(0)\n",
        "    elif params['regularization'] == 'l1_l2':\n",
        "        reg = l1_l2(l1=params['l1_reg'], l2=0)\n",
        "    else:\n",
        "        reg = None\n",
        "\n",
        "    model.add(Dense(params['neurons_layer_1'], activation='relu', input_shape=(x_train.shape[1],), kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_2'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(params['neurons_layer_3'], activation='relu', kernel_regularizer=reg))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Scaling y_train\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train_scaled, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1)\n",
        "\n",
        "    # Predict and evaluate on the test set\n",
        "    y_pred_scaled = model.predict(x_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "      # Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    # Root Mean Squared Error\n",
        "    rmse = np.sqrt(mse)\n",
        "    # R-squared Score\n",
        "    r2_score_value = r2_score(y_test, y_pred)\n",
        "    # Mean Absolute Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    # Mean Absolute Percentage Error\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    # Explained Variance Score\n",
        "    explained_variance = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    return {\"R2\": r2_score_value, \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape,\"explained\":explained_variance}\n",
        "\n",
        "\n",
        "optimal_params = {'learning_rate': 0.000371029603966771, 'neurons_layer_1': 944, 'neurons_layer_2': 160, 'neurons_layer_3': 144, 'batch_size': 128, 'epochs': 100, 'regularization': 'none'}\n",
        "\n",
        "performance_metrics = train_and_evaluate_model(x_train, y_train, x_test, y_test, optimal_params)\n",
        "print(\"Final Model Performance on Test Set:\", performance_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqEvl7c7EVdE",
        "outputId": "8e931e9c-ebdd-4a6f-b4c5-8b1b3906397c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "451/451 [==============================] - 8s 13ms/step - loss: 0.4042\n",
            "Epoch 2/100\n",
            "451/451 [==============================] - 6s 12ms/step - loss: 0.3248\n",
            "Epoch 3/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.2923\n",
            "Epoch 4/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.2745\n",
            "Epoch 5/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.2433\n",
            "Epoch 6/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.2496\n",
            "Epoch 7/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.2298\n",
            "Epoch 8/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.2047\n",
            "Epoch 9/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.2317\n",
            "Epoch 10/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.2045\n",
            "Epoch 11/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1862\n",
            "Epoch 12/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1933\n",
            "Epoch 13/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.1825\n",
            "Epoch 14/100\n",
            "451/451 [==============================] - 6s 12ms/step - loss: 0.1832\n",
            "Epoch 15/100\n",
            "451/451 [==============================] - 6s 12ms/step - loss: 0.1752\n",
            "Epoch 16/100\n",
            "451/451 [==============================] - 8s 19ms/step - loss: 0.1758\n",
            "Epoch 17/100\n",
            "451/451 [==============================] - 11s 25ms/step - loss: 0.1690\n",
            "Epoch 18/100\n",
            "451/451 [==============================] - 12s 26ms/step - loss: 0.1601\n",
            "Epoch 19/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.1759\n",
            "Epoch 20/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1566\n",
            "Epoch 21/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.1538\n",
            "Epoch 22/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.1601\n",
            "Epoch 23/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1503\n",
            "Epoch 24/100\n",
            "451/451 [==============================] - 11s 24ms/step - loss: 0.1404\n",
            "Epoch 25/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1469\n",
            "Epoch 26/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1314\n",
            "Epoch 27/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1427\n",
            "Epoch 28/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1480\n",
            "Epoch 29/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.1241\n",
            "Epoch 30/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1212\n",
            "Epoch 31/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.1223\n",
            "Epoch 32/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.1170\n",
            "Epoch 33/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1175\n",
            "Epoch 34/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.1245\n",
            "Epoch 35/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.1260\n",
            "Epoch 36/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1097\n",
            "Epoch 37/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.1071\n",
            "Epoch 38/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1040\n",
            "Epoch 39/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1043\n",
            "Epoch 40/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1107\n",
            "Epoch 41/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1263\n",
            "Epoch 42/100\n",
            "451/451 [==============================] - 8s 18ms/step - loss: 0.0999\n",
            "Epoch 43/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.1064\n",
            "Epoch 44/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.1131\n",
            "Epoch 45/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0975\n",
            "Epoch 46/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0904\n",
            "Epoch 47/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0943\n",
            "Epoch 48/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.1038\n",
            "Epoch 49/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0894\n",
            "Epoch 50/100\n",
            "451/451 [==============================] - 7s 17ms/step - loss: 0.0918\n",
            "Epoch 51/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0993\n",
            "Epoch 52/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.1017\n",
            "Epoch 53/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0970\n",
            "Epoch 54/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0850\n",
            "Epoch 55/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0807\n",
            "Epoch 56/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0804\n",
            "Epoch 57/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0833\n",
            "Epoch 58/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0828\n",
            "Epoch 59/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0819\n",
            "Epoch 60/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0790\n",
            "Epoch 61/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0789\n",
            "Epoch 62/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0872\n",
            "Epoch 63/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0754\n",
            "Epoch 64/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0756\n",
            "Epoch 65/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0750\n",
            "Epoch 66/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0749\n",
            "Epoch 67/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0726\n",
            "Epoch 68/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0705\n",
            "Epoch 69/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0705\n",
            "Epoch 70/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0831\n",
            "Epoch 71/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0838\n",
            "Epoch 72/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0708\n",
            "Epoch 73/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0656\n",
            "Epoch 74/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0622\n",
            "Epoch 75/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0643\n",
            "Epoch 76/100\n",
            "451/451 [==============================] - 8s 18ms/step - loss: 0.0646\n",
            "Epoch 77/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0608\n",
            "Epoch 78/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.0743\n",
            "Epoch 79/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0702\n",
            "Epoch 80/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0597\n",
            "Epoch 81/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0580\n",
            "Epoch 82/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0632\n",
            "Epoch 83/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0585\n",
            "Epoch 84/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0652\n",
            "Epoch 85/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0711\n",
            "Epoch 86/100\n",
            "451/451 [==============================] - 10s 21ms/step - loss: 0.0633\n",
            "Epoch 87/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0586\n",
            "Epoch 88/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0537\n",
            "Epoch 89/100\n",
            "451/451 [==============================] - 7s 15ms/step - loss: 0.0526\n",
            "Epoch 90/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0535\n",
            "Epoch 91/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0524\n",
            "Epoch 92/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0599\n",
            "Epoch 93/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0640\n",
            "Epoch 94/100\n",
            "451/451 [==============================] - 7s 17ms/step - loss: 0.0551\n",
            "Epoch 95/100\n",
            "451/451 [==============================] - 5s 12ms/step - loss: 0.0540\n",
            "Epoch 96/100\n",
            "451/451 [==============================] - 7s 16ms/step - loss: 0.0574\n",
            "Epoch 97/100\n",
            "451/451 [==============================] - 6s 14ms/step - loss: 0.0536\n",
            "Epoch 98/100\n",
            "451/451 [==============================] - 6s 13ms/step - loss: 0.0524\n",
            "Epoch 99/100\n",
            "451/451 [==============================] - 8s 17ms/step - loss: 0.0495\n",
            "Epoch 100/100\n",
            "451/451 [==============================] - 6s 12ms/step - loss: 0.0487\n",
            "451/451 [==============================] - 2s 3ms/step\n",
            "Final Model Performance on Test Set: {'R2': 0.8009808662236, 'MSE': 32024218000.0, 'RMSE': 178953.11, 'MAE': 82170.3, 'MAPE': 13.839609920978546, 'explained': 0.8017226457595825}\n"
          ]
        }
      ]
    }
  ]
}