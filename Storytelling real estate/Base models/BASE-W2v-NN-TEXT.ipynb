{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRhm0C0B5Atx",
        "outputId": "6605123c-68ce-42be-8e23-200c77ed1d11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Unnamed: 0',\n",
              " 'url',\n",
              " 'price',\n",
              " 'address',\n",
              " 'descrip',\n",
              " 'listed_since',\n",
              " 'zip_code',\n",
              " 'size',\n",
              " 'year',\n",
              " 'living_area',\n",
              " 'kind_of_house',\n",
              " 'building_type',\n",
              " 'num_of_rooms',\n",
              " 'num_of_bathrooms',\n",
              " 'layout',\n",
              " 'energy_label',\n",
              " 'insulation',\n",
              " 'heating',\n",
              " 'ownership',\n",
              " 'exteriors',\n",
              " 'parking',\n",
              " 'date_list',\n",
              " 'last_ask_price',\n",
              " 'last_ask_price_m2',\n",
              " 'city',\n",
              " 'log_id',\n",
              " 'num of tokens per descrip',\n",
              " 'descrip_en',\n",
              " 'numerical_price',\n",
              " 'numerical_price_per_m2',\n",
              " 'tag',\n",
              " 'house_category',\n",
              " 'living_area_float',\n",
              " 'size_float',\n",
              " 'zip_code_4_digits',\n",
              " 'postcode',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'processed_descrip',\n",
              " 'word2vec_embeddings']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\df-W2V.csv\")\n",
        "\n",
        "df.columns.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xOYgeGA653Zd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['url',\n",
              " 'price',\n",
              " 'address',\n",
              " 'descrip',\n",
              " 'listed_since',\n",
              " 'zip_code',\n",
              " 'size',\n",
              " 'year',\n",
              " 'living_area',\n",
              " 'kind_of_house',\n",
              " 'building_type',\n",
              " 'num_of_rooms',\n",
              " 'num_of_bathrooms',\n",
              " 'layout',\n",
              " 'energy_label',\n",
              " 'insulation',\n",
              " 'heating',\n",
              " 'ownership',\n",
              " 'exteriors',\n",
              " 'parking',\n",
              " 'date_list',\n",
              " 'last_ask_price',\n",
              " 'last_ask_price_m2',\n",
              " 'city',\n",
              " 'log_id',\n",
              " 'num of tokens per descrip',\n",
              " 'descrip_en',\n",
              " 'numerical_price',\n",
              " 'numerical_price_per_m2',\n",
              " 'tag',\n",
              " 'house_category',\n",
              " 'living_area_float',\n",
              " 'size_float',\n",
              " 'zip_code_4_digits',\n",
              " 'postcode',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'processed_descrip',\n",
              " 'word2vec_embeddings']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(df.columns[0], axis=1)\n",
        "df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lfUzX3wH5hrW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "\n",
        "df[\"word2vec_embeddings\"] = df[\"word2vec_embeddings\"].apply(literal_eval).apply(np.array)\n",
        "\n",
        "x = df[\"word2vec_embeddings\"]\n",
        "y = df['numerical_price']\n",
        "# Explode the embedding arrays into separate columns\n",
        "x = x.apply(pd.Series)\n",
        "\n",
        "# First split: separate out a test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syi2ZfQd6H3v",
        "outputId": "40083786-5e9a-457c-f918-7e0da064e3f3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate_nn(X_train, y_train):\n",
        "    # Basic hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    neurons_layer_1 = 1024\n",
        "    neurons_layer_2 = 512\n",
        "    neurons_layer_3 = 256\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "\n",
        "    # KFold Cross-validation\n",
        "    kf = KFold(n_splits=5)\n",
        "    r2_scores, mse_scores, rmse_scores, mape_scores, ev_scores = [], [], [], [], []\n",
        "\n",
        "    for train_index, test_index in kf.split(X_train):\n",
        "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "        # Build model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(neurons_layer_1, activation='relu', input_shape=(X_train_fold.shape[1],)))\n",
        "        model.add(Dense(neurons_layer_2, activation='relu'))\n",
        "        model.add(Dense(neurons_layer_3, activation='relu'))\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "        # Scaling y_train\n",
        "        scaler = StandardScaler()\n",
        "        y_train_scaled = scaler.fit_transform(y_train_fold.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train_fold, y_train_scaled, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Predict and evaluate\n",
        "        y_pred_scaled = model.predict(X_test_fold)\n",
        "        y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Metrics calculation\n",
        "        r2 = r2_score(y_test_fold, y_pred)\n",
        "        mse = mean_squared_error(y_test_fold, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = mean_absolute_percentage_error(y_test_fold, y_pred)\n",
        "        ev = explained_variance_score(y_test_fold, y_pred)\n",
        "\n",
        "        # Append scores\n",
        "        r2_scores.append(r2)\n",
        "        mse_scores.append(mse)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "        ev_scores.append(ev)\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_r2 = np.mean(r2_scores)\n",
        "    avg_mse = np.mean(mse_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "    avg_ev = np.mean(ev_scores)\n",
        "\n",
        "    return avg_r2, avg_mse, avg_rmse, avg_mape, avg_ev\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yGknqDde6T_b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (57611, 300)\n",
            "x_test shape: (14403, 300)\n",
            "y_train shape: (57611,)\n",
            "y_test shape: (14403,)\n",
            "Adjusted y_train shape: (57611,)\n",
            "Adjusted y_test shape: (14403,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert x_train and x_test to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "y_train = np.ravel(y_train)\n",
        "y_test = np.ravel(y_test)\n",
        "\n",
        "\n",
        "print(\"Adjusted y_train shape:\", y_train.shape)\n",
        "print(\"Adjusted y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt6U8jqL6ZZR",
        "outputId": "54eb2cbd-8549-4d5f-e0d9-ea86aae53c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "361/361 [==============================] - 1s 4ms/step\n",
            "361/361 [==============================] - 1s 3ms/step\n",
            "361/361 [==============================] - 1s 3ms/step\n",
            "361/361 [==============================] - 1s 3ms/step\n",
            "361/361 [==============================] - 1s 3ms/step\n",
            "Average R2 Score: 0.44960528183357684\n",
            "Average MSE: 95029468922.43176\n",
            "Average RMSE: 308019.91505848395\n",
            "Average MAPE: 0.33404983803015803\n",
            "Average Explained Variance: 0.4634388316694542\n"
          ]
        }
      ],
      "source": [
        "# Assuming x_train and y_train are already defined and preprocessed\n",
        "\n",
        "# Call the function with your data\n",
        "average_r2_score, average_mse, average_rmse, average_mape, average_ev = train_and_evaluate_nn(x_train, y_train)\n",
        "\n",
        "# Print the performance metrics\n",
        "print(\"Average R2 Score:\", average_r2_score)\n",
        "print(\"Average MSE:\", average_mse)\n",
        "print(\"Average RMSE:\", average_rmse)\n",
        "print(\"Average MAPE:\", average_mape)\n",
        "print(\"Average Explained Variance:\", average_ev)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
