{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYQmaW6nVUhI",
        "outputId": "eef432fd-d97c-4bbc-a1de-30d9989459c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# This will prompt for authorization to access your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the file path to the location where you uploaded your CSV in Google Drive\n",
        "file_path = '/content/drive/My Drive/Thesis/df-BERT_cat.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "df.columns.tolist()\n",
        "df = df.drop(df.columns[0], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "\n",
        "# Assuming df is your original DataFrame\n",
        "\n",
        "# Convert the 'ada_embedding_eng' column to a list of arrays if they are not already\n",
        "df[\"embeddings\"] = df[\"embeddings\"].apply(literal_eval).apply(np.array)\n",
        "\n",
        "# Separating the target variable and embeddings\n",
        "x = df[\"embeddings\"]\n",
        "y = df['numerical_price']\n",
        "\n",
        "# Dropping the 'ada_embedding_eng' column from df\n",
        "z = df.drop([\"embeddings\", \"numerical_price\"], axis=1)\n",
        "\n",
        "\n",
        "# Deleting original dataframe for memory purpose\n",
        "del df\n",
        "\n",
        "# Explode the embedding arrays into separate columns\n",
        "x = x.apply(pd.Series)\n",
        "\n",
        "# Concatenating the exploded embeddings with the rest of the data\n",
        "concatenated_df = pd.concat([x, z], axis=1).reset_index(drop=True)\n",
        "concatenated_df.columns = concatenated_df.columns.astype(str)"
      ],
      "metadata": {
        "id": "LKNjyqnCViMq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(concatenated_df, y, test_size=0.2, random_state=42)\n",
        "# Then, create and fit your scaler on the training data only"
      ],
      "metadata": {
        "id": "u1DiWCcGVveE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, explained_variance_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    # Scaling y_train\n",
        "    y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "\n",
        "    # Fitting the model\n",
        "    model.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "    # Predicting and inverse transformation\n",
        "    y_pred_scaled = model.predict(X_test)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "    ev = explained_variance_score(y_test, y_pred)\n",
        "\n",
        "    return r2, mse, rmse, mape, ev\n",
        "\n",
        "# Initialize LinearSVR with basic parameters\n",
        "clf_svr_base = LinearSVR(random_state=42, max_iter=10000)\n",
        "\n",
        "# Initialize StandardScaler for y\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# KFold Cross-validation\n",
        "kf = KFold(n_splits=5)\n",
        "r2_scores, mse_scores, rmse_scores, mape_scores, ev_scores = [], [], [], [], []\n",
        "\n",
        "for train_index, test_index in kf.split(x_train):\n",
        "\n",
        "    X_train_fold, X_test_fold = x_train.iloc[train_index], x_train.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    r2, mse, rmse, mape, ev = evaluate_model(clf_svr_base, X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n",
        "    r2_scores.append(r2)\n",
        "    mse_scores.append(mse)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "    ev_scores.append(ev)\n",
        "\n",
        "# Calculate and print average scores\n",
        "avg_r2 = np.mean(r2_scores)\n",
        "avg_mse = np.mean(mse_scores)\n",
        "avg_rmse = np.mean(rmse_scores)\n",
        "avg_mape = np.mean(mape_scores)\n",
        "avg_ev = np.mean(ev_scores)\n",
        "\n",
        "print(\"Average R2:\", avg_r2, \"Average MSE:\", avg_mse, \"Average RMSE:\", avg_rmse, \"Average MAPE:\", avg_mape, \"Average Explained Variance:\", avg_ev)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROuFeKk8VxF_",
        "outputId": "03d246c7-e168-4ce0-ad53-2da7d9e34c76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average R2: 0.5820916389756272 Average MSE: 72568928328.5253 Average RMSE: 268655.511679208 Average MAPE: 0.21051238829447555 Average Explained Variance: 0.5858188976097007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}