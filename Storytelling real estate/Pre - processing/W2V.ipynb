{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_model = pd.read_csv(r\"C:\\\\Users\\\\JellevanAs\\\\Documents\\\\Studie\\\\Thesis\\\\oude sets\\\\df-empty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Now, we'll download a pre-trained Word2Vec model using gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the Word2Vec model (this might take some time)\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'url', 'price', 'address', 'descrip', 'listed_since', 'zip_code', 'size', 'year', 'living_area', 'kind_of_house', 'building_type', 'num_of_rooms', 'num_of_bathrooms', 'layout', 'energy_label', 'insulation', 'heating', 'ownership', 'exteriors', 'parking', 'date_list', 'last_ask_price', 'last_ask_price_m2', 'city', 'log_id', 'num of tokens per descrip', 'descrip_en', 'numerical_price', 'numerical_price_per_m2', 'tag', 'house_category', 'living_area_float', 'size_float', 'zip_code_4_digits', 'postcode', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "print(df_model.columns.tolist())\n",
    "df_model = df_model.drop(df_model.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def numberToWords(n):\n",
    "    if not isinstance(n, int):\n",
    "        raise ValueError(\"Input must be an integer\")\n",
    "\n",
    "    # Handle negative numbers\n",
    "    if n < 0:\n",
    "        return \"minus \" + numberToWords(-n)\n",
    "\n",
    "    limit, t = 1000000000000, 0\n",
    "\n",
    "    # If zero, return 'zero'\n",
    "    if (n == 0):\n",
    "        return \"zero\"\n",
    "\n",
    "    # Arrays for various number parts\n",
    "    multiplier = [\"\", \"Trillion\", \"Billion\", \"Million\", \"Thousand\"]\n",
    "    first_twenty = [\"\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Eleven\", \"Twelve\", \"Thirteen\", \"Fourteen\", \"Fifteen\", \"Sixteen\", \"Seventeen\", \"Eighteen\", \"Nineteen\"]\n",
    "    tens = [\"\", \"Twenty\", \"Thirty\", \"Forty\", \"Fifty\", \"Sixty\", \"Seventy\", \"Eighty\", \"Ninety\"]\n",
    "\n",
    "    # If number is less than 20, return the corresponding word\n",
    "    if (n < 20):\n",
    "        return first_twenty[n]\n",
    "\n",
    "    answer = \"\"\n",
    "    i = n\n",
    "    while(i > 0):\n",
    "        curr_hun = i // limit\n",
    "\n",
    "        # Adjust for current multiplier\n",
    "        while (int(curr_hun) == 0):\n",
    "            i %= limit\n",
    "            limit /= 1000\n",
    "            curr_hun = i // limit\n",
    "            t += 1\n",
    "\n",
    "        # Add hundreds' place if applicable\n",
    "        if (int(curr_hun) > 99):\n",
    "                # Explicitly converting the index to an integer\n",
    "            hundreds_index = int(curr_hun) // 100\n",
    "            answer += (first_twenty[hundreds_index] + \" Hundred \")\n",
    "        # Adjust for tens and ones\n",
    "        curr_hun = int(curr_hun) % 100\n",
    "        if (int(curr_hun) > 0 and int(curr_hun) < 20):\n",
    "            answer += (first_twenty[int(curr_hun)] + \" \")\n",
    "        elif (int(curr_hun) % 10 == 0 and int(curr_hun) != 0):\n",
    "                index = int(int(curr_hun) // 10) - 1\n",
    "                answer += (tens[index] + \" \")\n",
    "        elif (int(curr_hun) > 19 and int(curr_hun) < 100):\n",
    "                tens_index = int(int(curr_hun) // 10) - 1\n",
    "                ones_index = int(int(curr_hun) % 10)\n",
    "                answer += (tens[tens_index] + \" \" + first_twenty[ones_index] + \" \")\n",
    "\n",
    "        # Add the current multiplier\n",
    "        if (t < 4):\n",
    "            answer += (multiplier[t] + \" \")\n",
    "\n",
    "        i = i % limit\n",
    "        limit = limit // 1000\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JellevanAs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "\n",
    "   # Convert numbers to words\n",
    "    def replace_with_words(match):\n",
    "        num = int(match.group())\n",
    "        return numberToWords(num)\n",
    "\n",
    "    text = re.sub(r'\\b\\d+\\b', replace_with_words, text)\n",
    "\n",
    "        # Replace punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('+', ' plus ')\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each description\n",
    "df_model['processed_descrip'] = df_model['descrip_en'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 72014/72014 [01:11<00:00, 1005.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for text in tqdm(df_model['processed_descrip'], desc=\"Processing Texts\"):\n",
    "    words = text.split()  # Split the text into words\n",
    "    word_embeddings = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]\n",
    "    \n",
    "    if word_embeddings:\n",
    "        # Average the embeddings of all words in the text\n",
    "        mean_embedding = np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        # If no words in the text are in the model's vocabulary, use a zero vector\n",
    "        mean_embedding = np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "    embeddings.append(mean_embedding.tolist())\n",
    "\n",
    "# Create a new column in the DataFrame for embeddings\n",
    "df_model['word2vec_embeddings'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 72014/72014 [00:08<00:00, 8052.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 37403038\n",
      "Embedded words: 32645623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "embedded_words = 0\n",
    "\n",
    "for text in tqdm(df_model['processed_descrip'], desc=\"Processing Texts\"):\n",
    "    words = text.split()  # Tokenize the text into words\n",
    "    total_words += len(words)  # Count total words\n",
    "    embedded_words += sum(word in word2vec_model.key_to_index for word in words)  # Count words in the model's vocabulary\n",
    "\n",
    "print(f\"Total words: {total_words}\")\n",
    "print(f\"Embedded words: {embedded_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.28067222774791\n"
     ]
    }
   ],
   "source": [
    "percentage_embedded = (embedded_words / total_words) * 100 if total_words > 0 else 0\n",
    "print(percentage_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv(\"word_2_vec.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
